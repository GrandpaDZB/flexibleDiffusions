from typing import Optional

import torch
import torch.nn as nn
import torch.nn.functional as F


class BasicClassifier(nn.Module):
    ''' 
    Basic classifier for classifier-guided diffusion. 
    The `y_type` argument can be either 'label' or 'logp', 
    which indicate the type of output generated by the classifier. 
    If `y_type` is set to 'label', then `n_categories` must be specified, 
    and the output of the classifier will be logits of size `n_categories`. 
    If `y_type` is set to 'logp', then the output of the classifier will be log-probabilities of size 1. 
    Strictly speaking, the output is log-probability plus a constant. 
    However, the constant will disappear when calculating the gradient of the log-probability.
    '''
    def __init__(self, 
        y_type: str = 'label',
        n_categories: Optional[int] = None,
    ):
        super().__init__()
        assert (y_type=='label') and (n_categories is not None)
        self.y_type = y_type
        self.n_categories = n_categories
        
    def forward(self, x: torch.Tensor, t: torch.Tensor) -> torch.Tensor:
        '''
        Input:
            - x: (batch, data_channels, ...)
            - t: (batch,)
        
        Output:
            - y: (batch, 1) for y_type='logp' or (batch, n_categories) for y_type='label'
        '''
        return torch.Tensor
    
    def loss(self, x: torch.Tensor, t: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
        '''
        Input:
            - x:    (batch, data_channels, ...)
            - t:    (batch,)
            - y:    (batch, 1) for y_type='logp' or (batch,) for y_type='label'
        
        Output:
            - loss:    (1,)
        '''
        if self.y_type == 'label':
            logits = self(x, t)
            return F.cross_entropy(logits, y)
        elif self.y_type == 'logp':
            logp_plus_contant = self(x, t)
            return F.mse_loss(logp_plus_contant, y)
        else:
            raise ValueError(f'Unknown y_type: {self.y_type}')
    
    def logp(self, x: torch.Tensor, t: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
        '''
        Input:
            - x:    (batch, data_channels, ...)
            - t:    (batch,)
            - y:    (batch, 1) for y_type='logp' or (batch,) for y_type='label'
        
        Output:
            - logp(y|x):    (batch, 1)
        '''
        if self.y_type == 'label':
            logits = self(x, t)
            logit = torch.gather(logits, dim=1, index=y.unsqueeze(1))
            logp = logit - torch.logsumexp(logits, dim=1, keepdim=True)
            return logp
        elif self.y_type == 'logp':
            logp_plus_contant = self(x, t)
            return logp_plus_contant
        else:
            raise ValueError(f'Unknown y_type: {self.y_type}')
    
    def gradients(self, x: torch.Tensor, t: torch.Tensor, y: torch.Tensor):
        x.requires_grad_()
        logp = self.logp(x, t, y)
        grad = torch.autograd.grad([logp.sum()], [x])[0]
        x.detach()
        return logp, grad